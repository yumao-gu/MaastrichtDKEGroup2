{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"OptComparison.ipynb","provenance":[],"authorship_tag":"ABX9TyMQyf+YNPxkerxQTLB8Asd6"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"bvlBBZDAEC3m","executionInfo":{"status":"ok","timestamp":1611150805470,"user_tz":-60,"elapsed":5394,"user":{"displayName":"Anna Koufou","photoUrl":"","userId":"11082672510994546978"}}},"source":["# Loading libraries\r\n","import numpy as np\r\n","import time\r\n","import matplotlib.pyplot as plt\r\n","import scipy as sp\r\n","from torch.distributions import uniform, normal\r\n","import random\r\n","import torch\r\n","import datetime\r\n","from scipy.stats import norm,multivariate_normal\r\n","import scipy.optimize\r\n","\r\n","def gaussian_mixture_model_sample(n_samples = 10000,\r\n","                                  means = [[0.9, -0.8],[-0.7, 0.9]],\r\n","                                  covs = [[[2.0, 0.3], [0.3, 0.5]],\r\n","                                          [[0.3, 0.5], [0.3, 2.0]]],\r\n","                                  weights = [0.3,0.7],\r\n","                                  vis = False,\r\n","                                  print_info = False):\r\n","\r\n","    '''\r\n","    Sample from a gaussian mixture model\r\n","\r\n","    Arguments:\r\n","        - n_samples: the num of sample points\r\n","        - means, covs: lists containing means and standard deviations of \\\r\n","                                        the single gaussian that are mixed. Note, the cov\\\r\n","                                        matrix must be semidefine\r\n","        - weights: weight of each used gaussian when combining\r\n","        - test: test flag, only test dim = 2\r\n","\r\n","    Outputs:\r\n","        - samples: Data matrix containing the samples from the gaussian mixture model\r\n","    '''\r\n","    # make sure all lists are o same length and weights are well defined\r\n","    start = time.time()\r\n","    np.random.seed()\r\n","    dim = len(means[0])\r\n","    samples = np.zeros((dim, n_samples))\r\n","\r\n","    dim = len(means[0])\r\n","    samples = np.zeros((dim,n_samples))\r\n","\r\n","    # sample from each gaussian and add up to obtain mixture model\r\n","    for i in range(n_samples):\r\n","        r = random.random()\r\n","        for j in range(len(weights)):\r\n","            if sum(weights[:j + 1]) > r:\r\n","                samples[:, i] = multivariate_normal.rvs(mean=means[j], cov=covs[j])\r\n","                break\r\n","\r\n","    if vis and dim == 2:\r\n","        x = np.array(samples[0, :])\r\n","        y = np.array(samples[1, :])\r\n","        plt.scatter(x, y, alpha=0.2, s=1)\r\n","        plt.show()\r\n","\r\n","    end = time.time()\r\n","    if print_info:\r\n","        print(f'gaussian_mixture_model_sample {end - start}')\r\n","\r\n","    return samples\r\n","\r\n","def phi_torch(x,mu,sigma):\r\n","    '''\r\n","    This function calculates the value of the density of a N(mu,sigma^2) gaussian random variable at point(s) x, only in a pytorch autograd compatible way\r\n","\r\n","    Arguments:\r\n","        - x: a torch tensor. The output inherits the dimensions of x, as the density is applied elementwise\r\n","        - mu: a torch tensor / as a scalar: expected value of gaussian random variable\r\n","        - sigma: a torch tensor / as a scalar: standard deviation of gaussian random variable\r\n","\r\n","    Output:\r\n","        - values of teh density at the provided x-values\r\n","        - as torch distributions deliver log_probs rather than probs we calculate prob = exp(log_prob)\r\n","\r\n","    Old Calculation: - return  1/(torch.sqrt(torch.tensor([2*np.pi]))*sigma)*torch.exp(-(x-mu)**2/(2*sigma**2))\r\n","                     - don't necessarily need this function anymore, but it helps keeping things manageable\r\n","\r\n","    '''\r\n","\r\n","    # Initializing normal distribution\r\n","    distribution = torch.distributions.normal.Normal(mu, sigma)\r\n","\r\n","    # Calculating log_probs\r\n","    log_prob = distribution.log_prob(x)\r\n","\r\n","    # Calculating and returning probs via: prob = exp(log_prob)\r\n","    prob = torch.exp(log_prob)\r\n","\r\n","    return prob\r\n","\r\n","def sigmoid(z):\r\n","    return (1+np.exp(-1*z))**(-1)\r\n","\r\n","def LogLikelihood(theta, x):\r\n","\r\n","    '''\r\n","    This is the (log-)likelihood function that is supposed to be fitted. Note that this is the (log-)likelihood w.r.t.\r\n","    one data point X_i. Thus, in later calculations like gradient ascent updates, the average of this function will be considered.\r\n","    This can easily achieved by torch.mean(LogLikelihood(theta, X)) where X is the complete data set (X_1, ..., X_n).\r\n","\r\n","    Info:\r\n","        - phi_torch(x, mu, sigma) gives the value of a density of a N(mu,sigma)-random variable at point x\r\n","\r\n","    Arguments:\r\n","        - theta: parameter vector that needs to altered to fit the model\r\n","        - x: a datapoint or a data set: the output will have the same dimensions as this\r\n","\r\n","    Output:\r\n","        - model log-likelihood of theta given x // element-wise on x\r\n","\r\n","    '''\r\n","\r\n","    g_theta = (1-theta[0, 0])*phi_torch(x, 0, 0.2) + theta[0, 0]*phi_torch(x, theta[0, 1], 0.2)\r\n","    log_g_theta = torch.log(g_theta)\r\n","\r\n","    return log_g_theta\r\n","\r\n","def gradient_ascent_torch(func, param, data, max_iterations, learningrate, run_id = 0,  print_info = False, a = 80, b = 20):\r\n","\r\n","    '''\r\n","    This function performs gradient ascent on the function func, which is governed by the arguments param.\r\n","\r\n","    Arguments:\r\n","        - func: function to be maximized\r\n","        - param: torch tensor with gradient; parameters that serve as arguments of func\r\n","        - data: data that governs/parametrizes func. #TODO One might change the design to give the data/X to the function globally\r\n","        - max_iterations: int; (maximum) number of iterations to be performed during gradient ascent\r\n","        - learningrate: scalar; learning rate / step size of the algorithm\r\n","        - run_id: tracker of how many runs of the procedure have been done\r\n","\r\n","    Outputs:\r\n","        - param: this (given convergence) is the argument of the maximum of func that was found.\r\n","        - loglikelihood_value: value of the found maximum\r\n","        - optim_trajectory: list of instances of param during optimization\r\n","    '''\r\n","\r\n","\r\n","    # starting time\r\n","    start = time.time()\r\n","\r\n","    # save initial parameter to trajectory\r\n","    with torch.no_grad(): # necessary?\r\n","        optim_trajectory = [param.clone().data.numpy()]\r\n","\r\n","    # Iterations\r\n","    for t in range(max_iterations):\r\n","\r\n","        # Evaluate loglikelihood of each data point: L(param | X_i) for all i\r\n","        loglikelihoods = func(param, data) # has dimension 1 x num_data_points\r\n","\r\n","        # Build mean of all log-likelihoods to get actual loglikelihood value\r\n","        loglikelihood_value = torch.mean(loglikelihoods) # has dim 1x1\r\n","\r\n","        # Calculate gradients of param\r\n","        loglikelihood_value.backward()\r\n","\r\n","        # Update param using gradient, save iterate and empty gradient for next calculation\r\n","        with torch.no_grad():\r\n","            param.add_(sigmoid((t-a)/b)*learningrate * param.grad)\r\n","            param.grad.zero_()\r\n","            optim_trajectory.append(param.clone().data.numpy())\r\n","\r\n","        # Keeping informed of progress during optimization\r\n","    end = time.time()\r\n","    if print_info:\r\n","        print(f'gradient_ascent_torch {end-start}')\r\n","\r\n","    # after all iterations are done return parameters, value of log-likelihood function at that maximum, trajectory\r\n","    return param, loglikelihood_value, optim_trajectory\r\n","\r\n","def gradient_ascent_torch2(func, param, data, accuracy, learningrate, run_id=0, print_info=False, a = 80, b = 20):\r\n","    '''\r\n","    This function performs gradient ascent on the function func, which is governed by the arguments param.\r\n","    Same as gradient_ascent_torch, only based on accuracy stooping criterion rather than maximum of iterations\r\n","\r\n","    Arguments:\r\n","        - func: function to be maximized\r\n","        - param: torch tensor with gradient; parameters that serve as arguments of func\r\n","        - data: data that governs/parametrizes func. #TODO One might change the design to give the data/X to the function globally\r\n","        - accuracy: float; stopping criterion: if two iterates are closer than this, the algorithm stops; should be chosen carefully under consideration of learningrate\r\n","        - learningrate: scalar; learning rate / step size of the algorithm\r\n","        - run_id: tracker of how many runs of the procedure have been done\r\n","\r\n","    Outputs:\r\n","        - param: this (given convergence) is the argument of the maximum of func that was found.\r\n","        - loglikelihood_value: value of the found maximum\r\n","        - optim_trajectory: list of instances of param during optimization\r\n","    '''\r\n","\r\n","    # starting time\r\n","    now = datetime.datetime.now()\r\n","\r\n","    # save initial parameter to trajectory\r\n","    with torch.no_grad():  # necessary?\r\n","        optim_trajectory = [param.clone().data.numpy()]\r\n","\r\n","    # Steps\r\n","    t = 0\r\n","    # Iterations\r\n","    while True:\r\n","\r\n","        # Evaluate loglikelihood of each data point: L(param | X_i) for all i\r\n","        loglikelihoods = func(param, data)  # has dimension 1 x num_data_points\r\n","\r\n","        # Build mean of all log-likelihoods to get actual loglikelihood value\r\n","        loglikelihood_value = torch.mean(loglikelihoods)  # has dim 1x1\r\n","\r\n","        # Calculate gradients of param\r\n","        loglikelihood_value.backward()\r\n","\r\n","        # Update param using gradient, save iterate and empty gradient for next calculation\r\n","        with torch.no_grad():\r\n","            param.add_(sigmoid((t-a)/b)*learningrate * param.grad)\r\n","            param.grad.zero_()\r\n","            optim_trajectory.append(param.clone().data.numpy())\r\n","\r\n","        # Keeping informed of progress during optimization\r\n","        if print_info:\r\n","            if t % 100 == 0:\r\n","                # TODO make more flexible for any ind of parameter length\r\n","                print(\r\n","                    f'Run: {run_id + 1}\\t| Iteration: {t} \\t| Log-Likelihood:{loglikelihood_value} \\t|  theta: {param}  |  Time needed: {datetime.datetime.now() - now}  ')\r\n","                now = datetime.datetime.now()\r\n","\r\n","        # Break off if accuracy is reached\r\n","        if np.linalg.norm(optim_trajectory[-1] - optim_trajectory[-2]) < accuracy:\r\n","            break\r\n","        # Updating step\r\n","        t += 1\r\n","        \r\n","    # after all iterations are done return parameters, value of log-likelihood function at that maximum, trajectory\r\n","  \r\n","    return param, loglikelihood_value, optim_trajectory\r\n","\r\n","def get_derivatives_torch(func, param, data, print_info = False):\r\n","\r\n","    '''\r\n","    This function serves to calculate all the desired derivatives needed in the creation of CIs. This is based on torch.autograd.functional\r\n","\r\n","    Arguments:\r\n","        - func: function of which the derivatives are to be calculated: this ought to be log(p(X_i | param)),\r\n","                that is function providing likelihood w.r.t. to each data point X_i\r\n","        - param: arguments of func, which are considered in the derivatives\r\n","        - data: data underlying teh log-likelihood function\r\n","        - print_dims: boolean whether to print dimensions of output or not // used for making suer dimensions are fitting\r\n","\r\n","    Output:\r\n","        - Scores: n x dim(param) matrix. Scores[i,j] = S_j(param|X_i) = \\nabla_{param_j}log(p(X_i | param))\r\n","        - Hessian: dim(param)x dim(param) matrix: Hessian[i,j] = \\nabla_{param_j}\\nabla_{param_i}  mean(log(p(X_s | param)), s=1,...,n)\r\n","\r\n","    Procedure:\r\n","        - func, as being log(p(X_i | param)) cannot directly be used. giving the whole dataset X to func the element-wise application gives\r\n","          func(param, X) of size (dim(data)=1 x n_samples). Thus, fixing this as a function of param (c.f. 'func_forScore') we have that\r\n","          Scores = \\nabla_{param} func(param, X) of (size n_samples x dim(param))\r\n","        - To calculate the hessian we need a scalar function we thus take the 'proper' log-likelihood function over the complete data set\r\n","          which is mean(log(p(X_s | param)) a function mapping from dim(param)->1.\r\n","          Thus, Hessian =  \\nabla\\nabla mean(log(p(X_s | param)), s=1,...,n) =  mean( \\nabla\\nabla log(p(X_s | param)), s=1,...,n),  as used in 'normal_CI'\r\n","\r\n","    '''\r\n","\r\n","    start = time.time()\r\n","    # Getting all scores w.r.t. the single X_i\r\n","    func_forScore = lambda args: func(args, data)\r\n","    Scores = torch.autograd.functional.jacobian(func_forScore, param).squeeze().squeeze()\r\n","\r\n","    # hessian needs a scalar function\r\n","    func_forHessian = lambda args: torch.mean(func(args, data))\r\n","    Hessian = torch.autograd.functional.hessian(func_forHessian, param).squeeze().squeeze()\r\n","\r\n","    end = time.time()\r\n","    if print_info:\r\n","        print(f'get_derivatives_torch {end-start}')\r\n","    return Scores, Hessian\r\n","\r\n","def theta_n_M(data, n_runs, func, max_iterations=1000, learningrate=0.01, print_info=False):\r\n","    '''\r\n","        This function performs gradient ascent on the function func, which is governed by the arguments param. Here this procedure is done with\r\n","        n_runs = M initializations. The GA limit with the highest Likelihood value is returned, i.e. theta_n_M\r\n","\r\n","        Arguments:\r\n","            - func: a pytorch autograd compatible function; function defining the logprobs that build the log-likelihood function (e.g. \\ref{func: LogLikelihood})\r\n","            - data: torch tensor of dim $k\\times n $ (c.f. section \\ref{sec: Data Generation});  these govern / parametrise func\r\n","            - max_iterations}: scalar (int); (maximum) number of iterations to be performed during gradient ascent\r\n","            - learningrate: scalar; learning rate / step size of the algorithm\r\n","            - print_info: Boolean; whether info about GA runs is to be printed or not\r\n","\r\n","        Outputs:\r\n","            - theta_hat: numpy arry of dim $1\\times d$; The estiamtor theta_n_M that is supposed to be the MLE\r\n","            - loglikelihood_value: value of the found maximum\r\n","            - optim_trajectory: list of instances of param during optimization\r\n","        '''\r\n","    # Initializing Loss as minus infinity to make sure first run achieves higher likelihood\r\n","    max_likelihood = -1 * np.inf\r\n","    # trajectory_dict is a cache to save the gradient ascent trajectory of all gradient ascent runs\r\n","    trajectory_dict = {}\r\n","\r\n","    # Running Gradient Ascent multiple (M=n_runs) times\r\n","    for run in range(n_runs):\r\n","\r\n","        # Create/ Initialize variable ' TODO: make initialization more flexible\r\n","        theta = torch.tensor([[uniform.Uniform(0., .6).sample(), uniform.Uniform(0., 5.).sample()]], requires_grad=True)\r\n","\r\n","        # Run complete Gradient ascent\r\n","        theta, L, trajectory = gradient_ascent_torch(func=func,\r\n","                                                     param=theta,\r\n","                                                     data=data,\r\n","                                                     max_iterations=max_iterations,\r\n","                                                     learningrate=learningrate,\r\n","                                                     run_id=run,\r\n","                                                     print_info=print_info)\r\n","        # print(f'{theta} {L}')\r\n","        # Save optimization trajectory\r\n","        trajectory_dict.update({run: trajectory})\r\n","        # Updating Quantities if new max is found\r\n","\r\n","        # compare likelihood value to previous runs\r\n","        if L > max_likelihood:\r\n","            # This takes forever if n is large. As it is torch implementation I don't see a way to get this faster\r\n","            # print(f'New Maximum found! old:{max_likelihood} -> new:{L}')\r\n","\r\n","            # Update highest likelihood and theta estimate\r\n","            max_likelihood = L\r\n","            theta_hat = theta.clone().data.numpy()\r\n","\r\n","    # Calculating Derivatives at found theta_hat\r\n","    # get derivatives\r\n","    # print(f'theta_n_M theta_hat {theta_hat}')\r\n","    theta_hat = torch.tensor(theta_hat, requires_grad = True)\r\n","\r\n","    return theta_hat, max_likelihood, trajectory_dict\r\n","\r\n","def theta_n_M2(data, n_runs, func, accuracy=0.00001, learningrate=0.01, print_info=False):\r\n","    '''\r\n","        This function performs gradient ascent on the function func, which is governed by the arguments param. Here this procedure is done with\r\n","        n_runs = M initializations. The GA limit with the highest Likelihood value is returned, i.e. theta_n_M\r\n","\r\n","        Arguments:\r\n","            - func: a pytorch autograd compatible function; function defining the logprobs that build the log-likelihood function (e.g. \\ref{func: LogLikelihood})\r\n","            - data: torch tensor of dim $k\\times n $ (c.f. section \\ref{sec: Data Generation});  these govern / parametrise func\r\n","            - max_iterations}: scalar (int); (maximum) number of iterations to be performed during gradient ascent\r\n","            - learningrate: scalar; learning rate / step size of the algorithm\r\n","            - print_info: Boolean; whether info about GA runs is to be printed or not\r\n","\r\n","        Outputs:\r\n","            - theta_hat: numpy arry of dim $1\\times d$; The estiamtor theta_n_M that is supposed to be the MLE\r\n","            - loglikelihood_value: value of the found maximum\r\n","            - optim_trajectory: list of instances of param during optimization\r\n","        '''\r\n","    # Initializing Loss as minus infinity to make sure first run achieves higher likelihood\r\n","    max_likelihood = -1 * np.inf\r\n","    # trajectory_dict is a cache to save the gradient ascent trajectory of all gradient ascent runs\r\n","    trajectory_dict = {}\r\n","\r\n","    # Running Gradient Ascent multiple (M=n_runs) times\r\n","    for run in range(n_runs):\r\n","\r\n","        # Create/ Initialize variable ' TODO: make initialization more flexible\r\n","        theta = torch.tensor([[uniform.Uniform(0., .6).sample(), uniform.Uniform(0., 5.).sample()]], requires_grad=True)\r\n","\r\n","        # Run complete Gradient ascent\r\n","        theta, L, trajectory = gradient_ascent_torch2(func=func,\r\n","                                                     param=theta,\r\n","                                                     data=data,\r\n","                                                     accuracy=accuracy,\r\n","                                                     learningrate=learningrate,\r\n","                                                     run_id=run,\r\n","                                                     print_info=print_info)\r\n","        # print(f'{theta} {L}')\r\n","        # Save optimization trajectory\r\n","        trajectory_dict.update({run: trajectory})\r\n","        # Updating Quantities if new max is found\r\n","\r\n","        # compare likelihood value to previous runs\r\n","        if L > max_likelihood:\r\n","            # This takes forever if n is large. As it is torch implementation I don't see a way to get this faster\r\n","            # print(f'New Maximum found! old:{max_likelihood} -> new:{L}')\r\n","\r\n","            # Update highest likelihood and theta estimate\r\n","            max_likelihood = L\r\n","            theta_hat = theta.clone().data.numpy()\r\n","\r\n","    # Calculating Derivatives at found theta_hat\r\n","    # get derivatives\r\n","    # print(f'theta_n_M theta_hat {theta_hat}')\r\n","    theta_hat = torch.tensor(theta_hat, requires_grad = True)\r\n","\r\n","    return theta_hat, max_likelihood, trajectory_dict\r\n","\r\n","def ConjugateGradient_FletcherReeves(theta,func,data,lr,it,conv,print_info=False):\r\n","  \r\n","  \r\n","  with torch.no_grad(): \r\n","    optim_trajectory = [theta.clone().data.numpy()]\r\n","\r\n","  err=10000\r\n","\r\n","  loglikelihoods = func(theta, data)\r\n","  loglikelihood_value = torch.mean(loglikelihoods)\r\n","  loglikelihood_value.backward()\r\n","  gradient = theta.grad\r\n","  gradientlist = [gradient.clone().data.numpy()]\r\n","  searchdirectionlist=[gradient.clone().data.numpy()]\r\n","\r\n","\r\n","  with torch.no_grad():\r\n","            theta.add_(lr * gradient)\r\n","            theta.grad.zero_()\r\n","            optim_trajectory.append(theta.clone().data.numpy())\r\n","  \r\n","  if print_info:\r\n","    print(f' Iteration: {0} \\t| Log-Likelihood:{loglikelihood_value} \\t|  theta: {theta}  \\t|Error:{err}')\r\n","  t=0  \r\n","  for i in range(it-1):\r\n","    loglikelihoods = func(theta, data)\r\n","\r\n","    loglikelihood_value = torch.mean(loglikelihoods)\r\n","\r\n","    loglikelihood_value.backward()\r\n","    gradient = theta.grad\r\n","    gradientlist.append(gradient.clone().data.numpy())\r\n","\r\n","\r\n","    #transpose for the multiplication\r\n","    grt=torch.transpose(gradient, 0, 1)\r\n","\r\n","    #FletcherReeves scalar\r\n","    previousgradient=torch.tensor(gradientlist[i])\r\n","    previousgrt=torch.transpose(previousgradient, 0, 1)\r\n","\r\n","    enm=torch.mm(gradient,grt)\r\n","    din=torch.mm(previousgradient,previousgrt)\r\n","    Beta=enm/din\r\n","\r\n","    #previous search direction times FR scalar\r\n","\r\n","    psd=torch.tensor(searchdirectionlist[i])\r\n","    addpart=psd*Beta\r\n","\r\n","    #search direction equivalent to the greadiand update in descent\r\n","    sd=torch.add(gradient,addpart)\r\n","    searchdirectionlist.append(sd.clone().data.numpy())\r\n","    #print(sd)\r\n","\r\n","    with torch.no_grad():\r\n","            theta.add_(lr * sd)\r\n","            theta.grad.zero_()\r\n","            optim_trajectory.append(theta.clone().data.numpy())\r\n","\r\n","    if print_info:\r\n","      if i % 1 == 0:\r\n","        now = datetime.datetime.now()\r\n","        print(f' Iteration: {i+1} \\t| Log-Likelihood:{loglikelihood_value} \\t|  theta: {theta}  \\t|Error:{err}  |  Time needed: {datetime.datetime.now()-now}  ')\r\n","    t += 1\r\n","    err=np.linalg.norm(optim_trajectory[-1] - optim_trajectory[-2])    \r\n","    if err < conv:\r\n","      break    \r\n","  return theta, loglikelihood_value, optim_trajectory\r\n","\r\n","def ConjugateGradient_PolakRibiere(theta,func,data,lr,it,conv,print_info=False):\r\n","  \r\n","  \r\n","  with torch.no_grad(): \r\n","    optim_trajectory = [theta.clone().data.numpy()]\r\n","\r\n","  err=10000\r\n","\r\n","  loglikelihoods = func(theta, data)\r\n","  loglikelihood_value = torch.mean(loglikelihoods)\r\n","  loglikelihood_value.backward()\r\n","  gradient = theta.grad\r\n","  gradientlist = [gradient.clone().data.numpy()]\r\n","  searchdirectionlist=[gradient.clone().data.numpy()]\r\n","\r\n","  with torch.no_grad():\r\n","            theta.add_(lr * gradient)\r\n","            theta.grad.zero_()\r\n","            optim_trajectory.append(theta.clone().data.numpy())\r\n","  \r\n","  if print_info:\r\n","    print(f' Iteration: {0} \\t| Log-Likelihood:{loglikelihood_value} \\t|  theta: {theta}  \\t|Error:{err}')\r\n","  t=0\r\n","  for i in range(it-1):\r\n","    loglikelihoods = func(theta, data)\r\n","\r\n","    loglikelihood_value = torch.mean(loglikelihoods)\r\n","\r\n","    loglikelihood_value.backward()\r\n","    \r\n","    gradient = theta.grad\r\n","    gradientlist.append(gradient.clone().data.numpy())\r\n","    \r\n","    \r\n","    #transpose for the multiplication\r\n","    grt=torch.transpose(gradient, 0, 1)\r\n","\r\n","    #Polak-Ribiere scalar\r\n","    previousgradient=torch.tensor(gradientlist[i])\r\n","    previousgrt=torch.transpose(previousgradient, 0, 1)\r\n","\r\n","    negpart=torch.add(gradient,-1*previousgradient)\r\n","    \r\n","    enm=torch.mm(negpart,grt)\r\n","    din=torch.mm(previousgradient,previousgrt)\r\n","    Beta=enm/din\r\n","\r\n","    #previous search direction times pr scalar\r\n","    psd=torch.tensor(searchdirectionlist[i])\r\n","    addpart=psd*Beta\r\n","\r\n","    #search direction equivalent to the greadiand update in descent\r\n","    sd=torch.add(gradient,addpart)\r\n","    searchdirectionlist.append(sd.clone().data.numpy())\r\n","\r\n","    with torch.no_grad():\r\n","            theta.add_(lr * sd)\r\n","            theta.grad.zero_()\r\n","            optim_trajectory.append(theta.clone().data.numpy())\r\n","\r\n","    if print_info:\r\n","      if i % 1 == 0:\r\n","        now = datetime.datetime.now()\r\n","        print(f' Iteration: {i+1} \\t| Log-Likelihood:{loglikelihood_value} \\t|  theta: {theta}  \\t|Error:{err}  |  Time needed: {datetime.datetime.now()-now}  ')\r\n","    t+=1\r\n","    err=np.linalg.norm(optim_trajectory[-1] - optim_trajectory[-2])    \r\n","    if err < conv:\r\n","      break    \r\n","  return theta, loglikelihood_value, optim_trajectory\r\n","\r\n","def theta_n_M_CG_PR(data, n_runs, func, max_iterations=1000, learningrate=0.01, print_info=False):\r\n","    '''\r\n","        This function performs gradient ascent on the function func, which is governed by the arguments param. Here this procedure is done with\r\n","        n_runs = M initializations. The GA limit with the highest Likelihood value is returned, i.e. theta_n_M\r\n","\r\n","        Arguments:\r\n","            - func: a pytorch autograd compatible function; function defining the logprobs that build the log-likelihood function (e.g. \\ref{func: LogLikelihood})\r\n","            - data: torch tensor of dim $k\\times n $ (c.f. section \\ref{sec: Data Generation});  these govern / parametrise func\r\n","            - max_iterations}: scalar (int); (maximum) number of iterations to be performed during gradient ascent\r\n","            - learningrate: scalar; learning rate / step size of the algorithm\r\n","            - print_info: Boolean; whether info about GA runs is to be printed or not\r\n","\r\n","        Outputs:\r\n","            - theta_hat: numpy arry of dim $1\\times d$; The estiamtor theta_n_M that is supposed to be the MLE\r\n","            - loglikelihood_value: value of the found maximum\r\n","            - optim_trajectory: list of instances of param during optimization\r\n","        '''\r\n","    # Initializing Loss as minus infinity to make sure first run achieves higher likelihood\r\n","    max_likelihood = -1 * np.inf\r\n","    # trajectory_dict is a cache to save the gradient ascent trajectory of all gradient ascent runs\r\n","    trajectory_dict = {}\r\n","\r\n","    # Running Gradient Ascent multiple (M=n_runs) times\r\n","    for run in range(n_runs):\r\n","\r\n","        # Run complete Gradient ascent\r\n","        theta = torch.tensor([[uniform.Uniform(0., .6).sample(),uniform.Uniform(0., 5.).sample()]], requires_grad = True)\r\n","        theta, L, trajectory = ConjugateGradient_PolakRibiere(theta,\r\n","                                                    func=func,\r\n","                                                     data=data,\r\n","                                                     lr=learningrate,\r\n","                                                     it=max_iterations,\r\n","                                                     conv=10**(-20),                                                     \r\n","                                                     print_info=print_info)\r\n","        # print(f'{theta} {L}')\r\n","        # Save optimization trajectory\r\n","        trajectory_dict.update({run: trajectory})\r\n","        # Updating Quantities if new max is found\r\n","\r\n","        # compare likelihood value to previous runs\r\n","        if L > max_likelihood:\r\n","            # This takes forever if n is large. As it is torch implementation I don't see a way to get this faster\r\n","            # print(f'New Maximum found! old:{max_likelihood} -> new:{L}')\r\n","\r\n","            # Update highest likelihood and theta estimate\r\n","            max_likelihood = L\r\n","            theta_hat = theta.clone().data.numpy()\r\n","\r\n","    # Calculating Derivatives at found theta_hat\r\n","    # get derivatives\r\n","    # print(f'theta_n_M_CG_PR theta_hat {theta_hat}')\r\n","    theta_hat = torch.tensor(theta_hat, requires_grad = True)\r\n","\r\n","    return theta_hat, max_likelihood, trajectory_dict\r\n","\r\n","def theta_n_M_CG_FR(data, n_runs, func, max_iterations=1000, learningrate=0.01, print_info=False):\r\n","    '''\r\n","        This function performs gradient ascent on the function func, which is governed by the arguments param. Here this procedure is done with\r\n","        n_runs = M initializations. The GA limit with the highest Likelihood value is returned, i.e. theta_n_M\r\n","\r\n","        Arguments:\r\n","            - func: a pytorch autograd compatible function; function defining the logprobs that build the log-likelihood function (e.g. \\ref{func: LogLikelihood})\r\n","            - data: torch tensor of dim $k\\times n $ (c.f. section \\ref{sec: Data Generation});  these govern / parametrise func\r\n","            - max_iterations}: scalar (int); (maximum) number of iterations to be performed during gradient ascent\r\n","            - learningrate: scalar; learning rate / step size of the algorithm\r\n","            - print_info: Boolean; whether info about GA runs is to be printed or not\r\n","\r\n","        Outputs:\r\n","            - theta_hat: numpy arry of dim $1\\times d$; The estiamtor theta_n_M that is supposed to be the MLE\r\n","            - loglikelihood_value: value of the found maximum\r\n","            - optim_trajectory: list of instances of param during optimization\r\n","        '''\r\n","    # Initializing Loss as minus infinity to make sure first run achieves higher likelihood\r\n","    max_likelihood = -1 * np.inf\r\n","    # trajectory_dict is a cache to save the gradient ascent trajectory of all gradient ascent runs\r\n","    trajectory_dict = {}\r\n","\r\n","    # Running Gradient Ascent multiple (M=n_runs) times\r\n","    for run in range(n_runs):\r\n","\r\n","        # Run complete Gradient ascent\r\n","        theta = torch.tensor([[uniform.Uniform(0., .4).sample(),uniform.Uniform(0., 4.).sample()]], requires_grad = True)\r\n","        theta, L, trajectory = ConjugateGradient_FletcherReeves(theta,func=func,\r\n","                                                     data=data,\r\n","                                                     lr=learningrate,\r\n","                                                     it=max_iterations,\r\n","                                                     conv=10**(-20),                                                     \r\n","                                                     print_info=print_info)\r\n","        # print(f'{theta} {L}')\r\n","\r\n","        # Save optimization trajectory\r\n","        trajectory_dict.update({run: trajectory})\r\n","        # Updating Quantities if new max is found\r\n","\r\n","        # compare likelihood value to previous runs\r\n","        if L > max_likelihood:\r\n","            # This takes forever if n is large. As it is torch implementation I don't see a way to get this faster\r\n","            # print(f'New Maximum found! old:{max_likelihood} -> new:{L}')\r\n","\r\n","            # Update highest likelihood and theta estimate\r\n","            max_likelihood = L\r\n","            theta_hat = theta.clone().data.numpy()\r\n","\r\n","    # Calculating Derivatives at found theta_hat\r\n","    # get derivatives\r\n","    # print(f'theta_n_M_CG_FR theta_hat {theta_hat}')\r\n","    theta_hat = torch.tensor(theta_hat, requires_grad = True)\r\n","\r\n","    return theta_hat, max_likelihood, trajectory_dict"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"LMmaBgJwEZtf"},"source":["import numpy as np\r\n","from scipy import stats\r\n","from scipy.stats import multivariate_normal\r\n","import torch\r\n","\r\n","def EMfromscratch(X,theta,it,con):\r\n","  x=X.numpy()\r\n","  mu,cov,rho=initial(theta)\r\n","  x=np.transpose(x)\r\n","  change=[1000,1000]\r\n","  for i in range(it):\r\n","    thetap=theta\r\n","    w=Estep(x,mu,cov,rho)\r\n","    mu,z=Mstep(x,w)\r\n","    theta=[z[1],mu[1]]\r\n","\r\n","    thetat=torch.tensor([theta])\r\n","    loglikelihoods = LogLikelihood(thetat, X)\r\n","    loglikelihood_value = torch.mean(loglikelihoods)\r\n","    for i in range(len(theta)):\r\n","      change[i]=theta[i]-thetap[i]\r\n","      error=np.linalg.norm(change)\r\n","      if con>error:\r\n","        return thetat,loglikelihood_value\r\n","  return thetat,loglikelihood_value\r\n","\r\n","def initial(theta):\r\n","  #mean initialization\r\n","  mu=[0.75,theta[1]]\r\n","  \r\n","  #cov based on paper\r\n","  cov=[0.2**2,0.2**2]\r\n","\r\n","  #initialixetion of latent variables\r\n","  rho=theta[0]\r\n","  return mu,cov,rho\r\n","  \r\n","def Estep(x,mu,cov,rho):\r\n","\r\n","  k=2\r\n","  pr=[]\r\n","  for i in range(k):\r\n","    pri=multivariate_normal.pdf(x,mu[i],cov[i])\r\n","    pr.append(pri)\r\n","  pr=np.array(pr)\r\n","\r\n","  #comp of latent variables=>wi=(pdf(mui,covi)*zi)/sum(pdf(mui,covi)*zi)\r\n","  multofpr=[]\r\n","  z=[1-rho,rho]\r\n","  \r\n","  for i in range(k):\r\n","    multofpr.append(z[i]*pr[i])\r\n","  multofpr=np.array(multofpr)\r\n","\r\n","  denom=np.zeros(x.shape[0])\r\n","\r\n","  for j in range(x.shape[0]):\r\n","    for i in range(k):\r\n","      denom[j]=denom[j]+multofpr[i][j]\r\n","\r\n","  w=np.zeros((k,x.shape[0]))\r\n","  \r\n","  for i in range(k):\r\n","    for j in range(x.shape[0]):\r\n","      w[i][j]=multofpr[i][j]/denom[j]\r\n","  return w\r\n","\r\n","def Mstep(x,w):\r\n","\r\n","  k=2\r\n","  d=w.shape[1]\r\n","  postw=np.zeros((1,k))\r\n","\r\n","  for i in range(k):\r\n","    for j in range(x.shape[0]):\r\n","      postw[0][i]=postw[0][i]+w[i][j]\r\n","  zq=postw/x.shape[0]\r\n","  z=zq[0]\r\n","\r\n","  \r\n","  mu=[0.75,0]\r\n","  wtr=w[1].reshape(len(x),1)\r\n","  numer=np.sum(wtr*x,axis=0)\r\n","  den=z[1]*d\r\n","  mutheta=numer/den\r\n","  mu[1]=mutheta[0]\r\n","\r\n","  return mu,z\r\n","\r\n","\r\n","def theta_n_M_EM(data, n_runs, max_iterations=1000):\r\n","\r\n","    # Initializing Loss as minus infinity to make sure first run achieves higher likelihood\r\n","    max_likelihood = -1 * np.inf\r\n","\r\n","\r\n","\r\n","    for run in range(n_runs):\r\n","        theta = np.array([uniform.Uniform(0., .4).sample().numpy(),uniform.Uniform(0., 4.).sample().numpy()])\r\n","\r\n","        # Run complete Gradient ascent\r\n","        theta, L = EMfromscratch(X=data,\r\n","                                  theta=theta,\r\n","                                  it=max_iterations,\r\n","                                  con=10**(-30))\r\n","        #print(f'{theta} {L}')\r\n","\r\n","        # Updating Quantities if new max is found\r\n","\r\n","        # compare likelihood value to previous runs\r\n","        if L > max_likelihood:\r\n","            # This takes forever if n is large. As it is torch implementation I don't see a way to get this faster\r\n","            # print(f'New Maximum found! old:{max_likelihood} -> new:{L}')\r\n","\r\n","            # Update highest likelihood and theta estimate\r\n","            max_likelihood = L\r\n","            theta_hat = theta.clone().data.numpy()\r\n","\r\n","    #print(f'theta_n_M_EM theta_hat {theta_hat}')\r\n","    theta_hat = torch.tensor(theta_hat, requires_grad = True)\r\n","\r\n","    return theta_hat, max_likelihood"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7rd9SVF0EaQu"},"source":["weights = [.5,.45,.05]\r\n","means = [[0.],[.75],[2.75]]\r\n","covs = [[.2**2],[.2**2],[.2**2]]\r\n","theta_gt = np.array([[0.43109772,1.0575168]])\r\n","get_data = lambda n: torch.from_numpy(gaussian_mixture_model_sample(n, means, covs, weights))\r\n","\r\n","data=get_data(int(1000))\r\n","conv=10**(-20)\r\n","it=100\r\n","lr=0.001\r\n","m=10\r\n","\r\n","print(\"FR\")\r\n","theta, lhv, _ = theta_n_M_CG_FR(data=data,\r\n","                                  n_runs=m,\r\n","                                  func=LogLikelihood,\r\n","                                  max_iterations=it,\r\n","                                  learningrate=lr,\r\n","                                  print_info=False)\r\n","print(theta)\r\n","print(lhv)\r\n","print()\r\n","print(\"PR\")\r\n","theta, lhv, _ = theta_n_M_CG_PR(data=data,\r\n","                               n_runs=m,\r\n","                               func=LogLikelihood,\r\n","                               max_iterations=it,\r\n","                               learningrate=lr,\r\n","                               print_info=False)\r\n","print(theta)\r\n","print(lhv)\r\n","print()\r\n","print(\"EM\")\r\n","theta, lhv= theta_n_M_EM(data = data,\r\n","                            n_runs = m,\r\n","                            max_iterations=it)\r\n","print(theta)\r\n","print(lhv)\r\n","print()\r\n","print(\"GAitt\")\r\n","theta, lhv, _ = theta_n_M(data=data,\r\n","                          n_runs=m,\r\n","                          func=LogLikelihood,\r\n","                          max_iterations=it,                       \r\n","                          learningrate=lr,\r\n","                          print_info=False)\r\n","print(theta)\r\n","print(lhv)\r\n","print()\r\n","print(\"GAacc\")\r\n","theta, lhv, _ = theta_n_M2(data=data,\r\n","                          accuracy=10**(-20),\r\n","                          n_runs=m,\r\n","                          func=LogLikelihood,\r\n","                          learningrate=lr,\r\n","                          print_info=False)\r\n","print(theta)\r\n","print(lhv)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vq_Orku7EdOO"},"source":["weights = [.5,.45,.05]\r\n","means = [[0.],[.75],[2.75]]\r\n","covs = [[.2**2],[.2**2],[.2**2]]\r\n","theta_gt = np.array([[0.43109772,1.0575168]])\r\n","get_data = lambda n: torch.from_numpy(gaussian_mixture_model_sample(n, means, covs, weights))\r\n","run=1\r\n","print_info=False\r\n","data=get_data(int(1000))\r\n","conv=10**(-20)\r\n","max_iterations=100\r\n","learningrate=0.001\r\n","theta = torch.tensor([[uniform.Uniform(0., .6).sample(), uniform.Uniform(0., 5.).sample()]], requires_grad=True)\r\n","theta_gt = theta[0].detach().numpy()\r\n","print(theta)\r\n","print(\"FR\")\r\n","thetae, L, trajectory = ConjugateGradient_FletcherReeves(theta,\r\n","                                                        func=LogLikelihood,\r\n","                                                     data=data,\r\n","                                                     lr=learningrate,\r\n","                                                     it=max_iterations,\r\n","                                                     conv=conv,                                                     \r\n","                                                     print_info=print_info)\r\n","\r\n","print(thetae)\r\n","print(L)\r\n","print()\r\n","print(\"PR\")\r\n","thetae, L, _ = ConjugateGradient_PolakRibiere(theta,\r\n","                                                    func=LogLikelihood,\r\n","                                                     data=data,\r\n","                                                     lr=learningrate,\r\n","                                                     it=max_iterations,\r\n","                                                     conv=conv,                                                     \r\n","                                                     print_info=print_info)\r\n","print(thetae)\r\n","print(L)\r\n","print()\r\n","print(\"EM\")\r\n","thetae, L = EMfromscratch(X=data,\r\n","                         theta=theta_gt,\r\n","                         it=max_iterations,\r\n","                         con=10**(-30))\r\n","print(thetae)\r\n","print(L)\r\n","print()\r\n","print(\"GAitt\")\r\n","thetae, L, _ = gradient_ascent_torch(func=LogLikelihood,\r\n","                                                     param=theta,\r\n","                                                     data=data,\r\n","                                                     max_iterations=max_iterations,\r\n","                                                     learningrate=learningrate,\r\n","                                                     print_info=print_info)\r\n","print(thetae)\r\n","print(L)\r\n","print()\r\n","print(\"GAacc\")\r\n","thetae, L, _ = gradient_ascent_torch2(func=LogLikelihood,\r\n","                                                     param=theta,\r\n","                                                     data=data,\r\n","                                                     accuracy=conv,\r\n","                                                     learningrate=learningrate,\r\n","                                                     print_info=print_info)\r\n","print(thetae)\r\n","print(L)"],"execution_count":null,"outputs":[]}]}